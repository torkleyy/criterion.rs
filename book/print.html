<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Criterion.rs Documentation</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="User Guide and Other Prose Documentation For Criterion.rs">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme -->
        

        

        <!-- Fetch Clipboard.js from CDN but have a local fallback -->
        <script src="https://cdn.jsdelivr.net/clipboard.js/1.6.1/clipboard.min.js"></script>
        <script>
            if (typeof Clipboard == 'undefined') {
                document.write(unescape("%3Cscript src='clipboard.min.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch store.js from local - TODO add CDN when 2.x.x is available on cdnjs -->
        <script src="store.js"></script>

    </head>
    <body class="light">
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = store.get('mdbook-theme');
            if (theme === null || theme === undefined) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = store.get('mdbook-sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li><a href="./criterion_rs.html"><strong>1.</strong> Criterion.rs</a></li><li><ul class="section"><li><a href="./getting_started.html"><strong>1.1.</strong> Getting Started</a></li></ul></li><li><a href="./user_guide/user_guide.html"><strong>2.</strong> User Guide</a></li><li><ul class="section"><li><a href="./user_guide/command_line.html"><strong>2.1.</strong> Command-Line Output</a></li><li><a href="./user_guide/plots_and_graphs.html"><strong>2.2.</strong> Plots &amp; Graphs</a></li><li><a href="./user_guide/benchmarking_with_inputs.html"><strong>2.3.</strong> Benchmarking With Inputs</a></li><li><a href="./user_guide/comparing_functions.html"><strong>2.4.</strong> Comparing Functions</a></li><li><a href="./user_guide/external_programs.html"><strong>2.5.</strong> Benchmarking External Programs</a></li></ul></li><li><a href="./analysis.html"><strong>3.</strong> Analysis Process</a></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page" tabindex="-1">
                
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars" title="Toggle sidebar"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush" title="Change theme"></i>
                    </div>

                    <h1 class="menu-title">Criterion.rs Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html">
                            <i id="print-button" class="fa fa-print" title="Print this book"></i>
                        </a>
                    </div>
                </div>

                <div id="content" class="content">
                    <a class="header" href="print.html#criterionrs" id="criterionrs"><h1>Criterion.rs</h1></a>
<p>Criterion.rs is a statistics-driven micro-benchmarking tool. It is a Rust port of <a href="https://hackage.haskell.org/package/criterion">Haskell's Criterion</a> library.</p>
<p>Criterion.rs benchmarks collect and store statistical information from run to run and can automatically detect performance regressions.</p>
<p>Criterion.rs is free and open source. You can find the source on <a href="https://github.com/japaric/criterion.rs">GitHub</a>. Issues and feature requests can be posted on <a href="https://github.com/japaric/criterion.rs/issues">the issue tracker</a>.</p>
<a class="header" href="print.html#api-docs" id="api-docs"><h2>API Docs</h2></a>
<p>In addition to this book, you may also wish to read <a href="http://japaric.github.io/criterion.rs/criterion/">the API documentation</a>.</p>
<a class="header" href="print.html#license" id="license"><h2>License</h2></a>
<p>Criterion.rs is dual-licensed under the <a href="https://github.com/japaric/criterion.rs/blob/master/LICENSE-APACHE">Apache 2.0</a> and the <a href="https://github.com/japaric/criterion.rs/blob/master/LICENSE-MIT">MIT</a> licenses.</p>
<a class="header" href="print.html#getting-started" id="getting-started"><h1>Getting Started</h1></a>
<p>Note that Criterion.rs requires a nightly version of Rust.</p>
<a class="header" href="print.html#step-1---add-dependency-to-cargotoml" id="step-1---add-dependency-to-cargotoml"><h3>Step 1 - Add Dependency to cargo.toml</h3></a>
<p>To enable Criterion.rs benchmarks, add the following to your <code>cargo.toml</code> file:</p>
<pre><code class="language-toml">[dev-dependencies]
criterion = &quot;0.1.1&quot;

[[bench]]
name = &quot;my_benchmark&quot;
harness = false
</code></pre>
<p>This adds a development dependency on Criterion.rs, and declares a benchmark called <code>my_benchmark</code> without the standard benchmarking harness. It's important to disable the standard benchmark harness, because we'll later add our own and we don't want them to conflict.</p>
<a class="header" href="print.html#step-2---add-benchmark" id="step-2---add-benchmark"><h3>Step 2 - Add Benchmark</h3></a>
<p>As an example, we'll benchmark an implementation of the Fibonacci function. Create a benchmark file at <code>$PROJECT/benches/my_benchmark.rs</code> with the following contents (see the Details section below for an explanation of this code):</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
#[macro_use]
extern crate criterion;

use criterion::Criterion;

fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n-1) + fibonacci(n-2),
    }
}

fn criterion_benchmark(c: &amp;mut Criterion) {
    c.bench_function(&quot;fib 20&quot;, |b| b.iter(|| fibonacci(20)));
}

criterion_group!(benches, criterion_benchmark);
criterion_main!(benches);
#}</code></pre></pre>
<a class="header" href="print.html#step-3---run-benchmark" id="step-3---run-benchmark"><h3>Step 3 - Run Benchmark</h3></a>
<p>To run this benchmark, use the following command:</p>
<p><code>cargo bench</code></p>
<p>You should see output similar to this:</p>
<pre><code>     Running target\release\deps\criterion_example-c6a3683ae7e18b5a.exe

running 1 test
Gnuplot not found, disabling plotting
Benchmarking fib 20
&gt; Warming up for 3.0000 s
&gt; Collecting 100 samples in estimated 5.0726 s
&gt; Found 11 outliers among 99 measurements (11.11%)
  &gt; 2 (2.02%) high mild
  &gt; 9 (9.09%) high severe
&gt; Performing linear regression
  &gt;  slope [26.778 us 27.139 us]
  &gt;    R^2  0.8382863 0.8358049
&gt; Estimating the statistics of the sample
  &gt;   mean [26.913 us 27.481 us]
  &gt; median [26.706 us 26.910 us]
  &gt;    MAD [276.37 ns 423.53 ns]
  &gt;     SD [729.17 ns 2.0625 us]

test criterion_benchmark ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre>
<a class="header" href="print.html#details" id="details"><h3>Details</h3></a>
<p>Let's go back and walk through that benchmark code in more detail.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
#[macro use]
extern crate criterion;

use criterion::Criterion;
#}</code></pre></pre>
<p>First, we declare the criterion crate and import the <a href="http://japaric.github.io/criterion.rs/criterion/struct.Criterion.html">Criterion type</a>. Criterion is the main type for the Criterion.rs library. It provides methods to configure and define groups of benchmarks.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n-1) + fibonacci(n-2),
    }
}
#}</code></pre></pre>
<p>Second, we define the function to benchmark. In normal usage, this would be imported from elsewhere in your crate, but for simplicity we'll just define it right here.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn criterion_benchmark(c: &amp;mut Criterion) {
#}</code></pre></pre>
<p>Here we create a function to contain our benchmark code. The name of the benchmark function doesn't matter, but it should be clear and understandable.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
    c.bench_function(&quot;fib 20&quot;, |b| b.iter(|| fibonacci(20)));
}
#}</code></pre></pre>
<p>This is where the real work happens. The <code>bench_function</code> method defines a benchmark with a name and a closure. The name should be unique among all of the benchmarks for your project. The closure must accept one argument, a <a href="http://japaric.github.io/criterion.rs/criterion/struct.Bencher.html">Bencher</a>. The bencher performs the benchmark - in this case, it simply calls our <code>fibonacci</code> function in a loop. There are a number of other benchmark functions, including the option to benchmark with arguments, to benchmark external programs and to compare the performance of two functions. See the API documentation for details on all of the different benchmarking options.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
criterion_group!(benches, criterion_benchmark);
criterion_main!(benches);
#}</code></pre></pre>
<p>Here we invoke the <code>criterion_group!</code> <a href="http://japaric.github.io/criterion.rs/criterion/macro.criterion_group.html">(link)</a> macro to generate a benchmark group called benches, containing the <code>criterion_benchmark</code> function defined earlier. Finally, we invoke the <code>criterion_main!</code> <a href="http://japaric.github.io/criterion.rs/criterion/macro.criterion_main.html">(link)</a> macro to generate a main function which executes the <code>benches</code> group. See the API documentation for more information on these macros.</p>
<a class="header" href="print.html#step-4---optimize" id="step-4---optimize"><h3>Step 4 - Optimize</h3></a>
<p>This fibonacci function is quite inefficient. We can do better:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn fibonacci(n: u64) -&gt; u64 {
    let mut a = 0u64;
    let mut b = 1u64;
    let mut c = 0u64;

    if n == 0 {
        return 0
    }

    for _ in 0..(n+1) {
        c = a + b;
        a = b;
        b = c;
    }
    return b;
}
#}</code></pre></pre>
<p>Running the benchmark now produces output like this:</p>
<pre><code>     Running target\release\deps\criterion_example-c6a3683ae7e18b5a.exe

running 1 test
Gnuplot not found, disabling plotting
Benchmarking fib 20
&gt; Warming up for 3.0000 s
&gt; Collecting 100 samples in estimated 5.0000 s
&gt; Found 9 outliers among 99 measurements (9.09%)
  &gt; 5 (5.05%) high mild
  &gt; 4 (4.04%) high severe
&gt; Performing linear regression
  &gt;  slope [428.43 ps 456.05 ps]
  &gt;    R^2  0.2214335 0.2189765
&gt; Estimating the statistics of the sample
  &gt;   mean [431.59 ps 461.16 ps]
  &gt; median [403.16 ps 419.31 ps]
  &gt;    MAD [6.6660 ps 28.954 ps]
  &gt;     SD [53.404 ps 94.558 ps]
fib 20: Comparing with previous sample
&gt; Performing a two-sample t-test
  &gt; H0: Both samples have the same mean
  &gt; p = 0
  &gt; Strong evidence to reject the null hypothesis
&gt; Estimating relative change of statistics
  &gt;   mean [-99.998% -99.998%]
  &gt; median [-99.998% -99.998%]
  &gt; mean has improved by 100.00%
  &gt; median has improved by 100.00%

test criterion_benchmark ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre>
<p>As you can see, Criterion is statistically confident that our optimization has made an improvement. If we introduce a performance regression, Criterion will instead fail the test by panicking.</p>
<a class="header" href="print.html#user-guide" id="user-guide"><h1>User Guide</h1></a>
<p>This chapter covers the output produced by Criterion.rs benchmarks, both the command-line reports and the charts. It also details more advanced usages of Criterion.rs such as benchmarking external programs and comparing the performance of multiple functions.</p>
<a class="header" href="print.html#command-line-output" id="command-line-output"><h1>Command-Line Output</h1></a>
<p>Every Criterion.rs benchmark calculates statistics from the measured iterations and produces a report like this:</p>
<pre><code>Benchmarking alloc
&gt; Warming up for 1.0000 s
&gt; Collecting 100 samples in estimated 31.601 s
&gt; Found 11 outliers among 99 measurements (11.11%)
  &gt; 4 (4.04%) high mild
  &gt; 7 (7.07%) high severe
&gt; Performing linear regression
  &gt;  slope [5.9892 ms 6.0237 ms]
  &gt;    R^2  0.9821950 0.9812767
&gt; Estimating the statistics of the sample
  &gt;   mean [5.9994 ms 6.1268 ms]
  &gt; median [5.9781 ms 5.9935 ms]
  &gt;    MAD [16.247 us 32.562 us]
  &gt;     SD [61.936 us 538.86 us]
</code></pre>
<a class="header" href="print.html#warmup" id="warmup"><h2>Warmup</h2></a>
<p>Every Criterion.rs benchmark iterates the benchmarked function automatically for a configurable warmup period (by default, for one second). For Rust function benchmarks, this is to warm up the processor caches and (if applicable) file system caches. For external program benchmarks, it can also be used to warm up JIT compilers.</p>
<a class="header" href="print.html#collecting-samples" id="collecting-samples"><h2>Collecting Samples</h2></a>
<p>Criterion iterates the function to be benchmarked with a varying number of iterations to generate an estimate of the time taken by each iteration. The number of samples is configurable. It also prints an estimate of the time the sampling process will take based on the time per iteration during the warmup period.</p>
<a class="header" href="print.html#detecting-outliers" id="detecting-outliers"><h2>Detecting Outliers</h2></a>
<p>Criterion.rs attempts to detect unusually high or low samples and reports them as outliers. A large number of outliers suggests that the benchmark results are noisy and should be viewed with appropriate skepticism. In this case, you can see that there are some samples which took much longer than normal. This might be caused by unpredictable load on the computer running the benchmarks, thread or process scheduling, or irregularities in the time taken by the code being benchmarked.</p>
<p>In order to ensure reliable results, benchmarks should be run on a quiet computer and should be designed to do approximately the same amount of work for each iteration. If this is not possible, consider increasing the sample size and/or measurement time to reduce the influence of outliers on the results at the cost of longer benchmarking time. Alternately, the warmup period can be extended (to ensure that any JIT compilers or similar are warmed up) or other iteration loops can be used to perform setup before each benchmark to prevent that from affecting the results.</p>
<a class="header" href="print.html#linear-regression" id="linear-regression"><h2>Linear Regression</h2></a>
<pre><code>&gt; Performing linear regression
  &gt;  slope [5.9892 ms 6.0237 ms]
  &gt;    R^2  0.9821950 0.9812767
</code></pre>
<p>Here, Criterion.rs attempts to calculate the time taken per iteration of the benchmark.</p>
<p>The slope represents Criterion.rs' best guess at the time taken for each iteration of the benchmark. More precisely, this shows a 95% confidence interval on the time per iteration. The confidence level is configurable. A greater confidence level (eg. 99%) will widen the interval and thus provide the user with less information about the true slope. On the other hand, a lesser confidence interval (eg. 90%) will narrow the interval but then the user is less confident that the interval contains the true slope. 95% is generally a good balance.</p>
<p>The R^2 line indicates how accurately the linear model fits the measurements. If the measurements aren't too noisy and the benchmark is performing the same amount of work for each iteration, this number should be very close to 1.0. If it is not, the benchmark results may be unreliable.</p>
<a class="header" href="print.html#estimating-statistics" id="estimating-statistics"><h2>Estimating Statistics</h2></a>
<pre><code>&gt; Estimating the statistics of the sample
  &gt;   mean [5.9994 ms 6.1268 ms]
  &gt; median [5.9781 ms 5.9935 ms]
  &gt;    MAD [16.247 us 32.562 us]
  &gt;     SD [61.936 us 538.86 us]
</code></pre>
<p>Criterion.rs performs <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap resampling</a> to estimate some important statistics about the samples collected. The number of bootstrap samples is configurable, and defaults to 100,000.</p>
<a class="header" href="print.html#meanmedian" id="meanmedian"><h4>Mean/Median</h4></a>
<p>These lines show a confidence interval on the mean and median values of the sample and give a good estimate of how long you can expect one iteration of the benchmark to take. The mean and median values should be quite close - if they are not, the benchmark may be unreliable due to a high number of outliers.</p>
<a class="header" href="print.html#madsd" id="madsd"><h4>MAD/SD</h4></a>
<p>These lines report the <a href="https://en.wikipedia.org/wiki/Median_absolute_deviation">Median Absolute Deviation</a> and <a href="https://en.wikipedia.org/wiki/Standard_deviation">Standard Deviation</a> of the sample. This is another indicator of noise in the benchmark. In a good benchmark, the MAD and SD will be relatively small compared to the mean and median. Large MAD or SD values indicate that there is a great deal of noise in the sample and the results may be unreliable or Criterion.rs may be unable to detect small optimizations or regressions.</p>
<a class="header" href="print.html#comparing-to-previous-sample" id="comparing-to-previous-sample"><h2>Comparing To Previous Sample</h2></a>
<p>When a Criterion.rs benchmark is run, it saves statistical information in the <code>.criterion</code> directory. Subsequent executions of the benchmark will load this data and compare it with the current sample to show the effects of changes in the code.</p>
<pre><code>alloc: Comparing with previous sample
&gt; Performing a two-sample t-test
  &gt; H0: Both samples have the same mean
  &gt; p = 0
  &gt; Strong evidence to reject the null hypothesis
&gt; Estimating relative change of statistics
  &gt;   mean [-34.062% -31.916%]
  &gt; median [-33.186% -32.662%]
  &gt; mean has improved by 33.00%
  &gt; median has improved by 32.84%
</code></pre>
<p>First, Criterion.rs performs a statistical test to see whether the performance has changed significantly between runs (indicated by the 'Strong evidence...' line). Then it will proceed to estimate by how much the performance has changed. In this case, the performance has improved significantly. The <code>p = 0</code> line is an indicator of the chances that the observed differences in mean iteration time could have happened by chance alone. If the difference in means is large compared to the noise (indicated by p being close to zero) then it is likely there is a true difference in performance between the benchmarking runs.</p>
<pre><code>alloc: Comparing with previous sample
&gt; Performing a two-sample t-test
  &gt; H0: Both samples have the same mean
  &gt; p = 0.56624
  &gt; Can't reject the null hypothesis
&gt; Estimating relative change of statistics
  &gt;   mean [-2.1989% +1.3191%]
  &gt; median [-1.0305% -0.0042%]
</code></pre>
<p>In this example, the change in iteration time is much smaller. Note that p is much larger than zero and Criterion.rs reports that it can't reject the null hypothesis (ie. that it can't be confident the different is due to a true change in performance rather than chance). The threshold between 'Strong evidence...' and 'Can't reject...' is configurable, and defaults to 0.05. Increasing this threshold allows Criterion to detect smaller changes in noisier data at the cost of a greater chance of false positives.</p>
<pre><code>alloc: Comparing with previous sample
&gt; Performing a two-sample t-test
  &gt; H0: Both samples have the same mean
  &gt; p = 0
  &gt; Strong evidence to reject the null hypothesis
&gt; Estimating relative change of statistics
  &gt;   mean [+54.245% +62.569%]
  &gt; median [+49.228% +50.627%]
  &gt; mean has regressed by 58.13%
  &gt; median has regressed by 49.80%
thread 'alloc' panicked at 'alloc has regressed', src/analysis/compare.rs:58:8
note: Run with `RUST_BACKTRACE=1` for a backtrace.
test alloc ... FAILED
</code></pre>
<p>In this example, the performance has regressed significantly. When Criterion.rs is confident of a performance regression, it panics in order to fail the test.</p>
<a class="header" href="print.html#a-note-of-caution" id="a-note-of-caution"><h2>A Note Of Caution</h2></a>
<p>Criterion.rs is designed to produce robust statistics when possible, but it can't account for everything. For example, the performance improvements and regressions listed in the above examples were created just by switching my laptop between battery power and wall power rather than changing the code under test. Care must be taken to ensure that benchmarks are performed under similar conditions in order to produce meaningful results.</p>
<a class="header" href="print.html#plots--graphs" id="plots--graphs"><h1>Plots &amp; Graphs</h1></a>
<p>If <a href="http://www.gnuplot.info/">gnuplot</a> is installed, Criterion.rs can generate a number of useful charts and graphs which you can check to get a better understanding of the behavior of the benchmark.</p>
<a class="header" href="print.html#file-structure" id="file-structure"><h2>File Structure</h2></a>
<p>The plots and saved data are stored under <code>.criterion/$BENCHMARK_NAME/</code>. Here's an example of the folder structure:</p>
<pre><code>$BENCHMARK/
├── base/
│  ├── estimates.json
│  ├── MAD.svg
│  ├── mean.svg
│  ├── median.svg
│  ├── pdf.svg
│  ├── regression.svg
│  ├── sample.json
│  ├── SD.svg
│  ├── slope.svg
│  └── tukey.json
├── both/
│  ├── pdf.svg
│  └── regression.svg
├── change/
│  ├── estimates.json
│  ├── mean.svg
│  ├── median.svg
│  └── t-test.svg
└── new/
   ├── estimates.json
   ├── MAD.svg
   ├── mean.svg
   ├── median.svg
   ├── pdf.svg
   ├── regression.svg
   ├── sample.json
   ├── SD.svg
   ├── slope.svg
   └── tukey.json
</code></pre>
<p>The <code>new</code> folder contains the statistics and plots for the last benchmarking run, while the <code>base</code> folder contains those for the previous run. Criterion.rs only keeps historical data for two runs. The <code>both</code> folder contains plots which show both runs on one plot, while the <code>change</code> folder contains plots showing the differences between the two runs. This example shows the plots produced by the default <code>bench_function</code> benchmark method. Other methods may produce additional charts, which will be detailed in their respective pages.</p>
<a class="header" href="print.html#madmeanmediansdslope" id="madmeanmediansdslope"><h2>MAD/Mean/Median/SD/Slope</h2></a>
<p><img src="./user_guide/mean.svg" alt="Mean Chart" /></p>
<p>These are the simplest of the plots generated by Criterion.rs. They display the bootstrapped distributions and confidence intervals for the given statistics.</p>
<a class="header" href="print.html#regression" id="regression"><h2>Regression</h2></a>
<p><img src="./user_guide/regression.svg" alt="Regression Chart" /></p>
<p>The regression plot shows each data point plotted on an X-Y plane showing the number of iterations vs the time taken. It also shows the line representing Criterion.rs' best guess at the time per iteration. A good benchmark will show the data points all closely following the line. If the data points are scattered widely, this indicates that there is a lot of noise in the data and that the benchmark may not be reliable. If the data points follow a consistent trend but don't match the line (eg. if they follow a curved pattern or show several discrete line segments) this indicates that the benchmark is doing different amounts of work depending on the number of iterations, which prevents Criterion.rs from generating accurate statistics and means that the benchmark may need to be reworked.</p>
<p>The combined regression plot in the <code>both</code> folder shows only the regression lines and is a useful visual indicator of the difference in performance between the two runs.</p>
<a class="header" href="print.html#pdf" id="pdf"><h2>PDF</h2></a>
<p><img src="./user_guide/pdf.svg" alt="PDF Chart" /></p>
<p>The PDF chart shows the probability distribution function for the samples. It also shows the ranges used to classify samples as outliers. In this example (as in the regression example above) we can see that the performance trend changes noticeably below ~35 iterations, which we may wish to investigate.</p>
<a class="header" href="print.html#benchmarking-with-inputs" id="benchmarking-with-inputs"><h1>Benchmarking With Inputs</h1></a>
<p>Criterion.rs can run benchmarks with multiple different input values to investigate how the performance behavior changes with different inputs.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
    static KB: usize = 1024;

    Criterion::default()
    .bench_function_over_inputs(&quot;from_elem&quot;, |b, &amp;&amp;size| {
        b.iter(|| iter::repeat(0u8).take(size).collect::&lt;Vec&lt;_&gt;&gt;());
    }, &amp;[KB, 2 * KB, 4 * KB, 8 * KB, 16 * KB])
#}</code></pre></pre>
<p>In this example, we're benchmarking the time it takes to collect a iterator producing a sequence of N bytes into a Vec. We use the <code>bench_function_over_inputs</code> method. Unlike <code>bench_function</code>, the lambda here takes a Bencher and a reference to a parameter, in this case <code>size</code>. Finally, we provide a slice of potential input values. This generates five benchmarks, named &quot;from_elem/1024&quot; through &quot;from_elem/16384&quot; which individually behave the same as any other benchmark. Criterion.rs also generates some charts in <code>.criterion/$BENCHMARK/summary/</code> showing how the iteration time changes as a function of the input.</p>
<p><img src="./user_guide/means.svg" alt="Means Chart" /></p>
<p>Here we can see that there is a approximately-linear relationship between the length of an iterator and the time taken to collect it into a Vec.</p>
<a class="header" href="print.html#comparing-functions" id="comparing-functions"><h1>Comparing Functions</h1></a>
<p>Criterion.rs can automatically benchmark multiple implementations of a function and produce summary graphs to show the differences in performance between them. First, lets create a comparison benchmark.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn fibonacci_slow(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci_slow(n-1) + fibonacci_slow(n-2),
    }
}

fn fibonacci_fast(n: u64) -&gt; u64 {
    let mut a = 0u64;
    let mut b = 1u64;
    let mut c = 0u64;

    if n == 0 {
        return 0
    }

    for _ in 0..(n+1) {
        c = a + b;
        a = b;
        b = c;
    }
    return b;
}
#}</code></pre></pre>
<p>These are the same two fibonacci functions from the <a href="./getting_started.html">Getting Started</a> page. The difference here is that we import the <a href="http://japaric.github.io/criterion.rs/criterion/struct.Fun.html">Fun type</a> as well.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn fibonaccis(c: &amp;mut Criterion) {
    let fib_slow = Fun::new(&quot;Recursive&quot;, |b, i| b.iter(|| fibonacci_slow(*i)));
    let fib_fast = Fun::new(&quot;Iterative&quot;, |b, i| b.iter(|| fibonacci_fast(*i)));
#}</code></pre></pre>
<p>Here, we create two benchmark functions which simply call our two Fibonacci implementations. Notice that the closure takes two arguments - b is the Bencher as in other examples, and i is the input parameter to be given to the benchmarked function.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
    let functions = vec!(fib_slow, fib_fast);
    
    c.bench_functions(&quot;Fibonacci&quot;, functions, &amp;20);
}
#}</code></pre></pre>
<p>Finally, we construct a Vec of the benchmark functions and run the benchmark. This performs two benchmarks (&quot;Fibonacci/Recursive&quot; and &quot;Fibonacci/Iterative&quot;) which individually behave the same as other benchmarks seen earlier. In addition to the usual set of plots generated for each individual benchmark, this will generate a set of summary plots at <code>.criterion/$BENCHMARK/Summary</code> highlighting the differences between the functions.</p>
<a class="header" href="print.html#meansmediansslopes" id="meansmediansslopes"><h2>Means/Medians/Slopes</h2></a>
<p><img src="./user_guide/means_compare.svg" alt="Means Chart" /></p>
<p>These charts show the absolute and relative differences in the appropriate statistics between the implementations. Here we can see that the recursive implementation took 40 microseconds per iteration, which is 1665 times slower than the iterative implementation.</p>
<a class="header" href="print.html#violin-plot" id="violin-plot"><h2>Violin Plot</h2></a>
<p><img src="./user_guide/violin_plot.svg" alt="Violin Plot" /></p>
<p>The <a href="https://en.wikipedia.org/wiki/Violin_plot">Violin Plot</a> shows the median times and the PDF of each implementation.</p>
<a class="header" href="print.html#benchmarking-external-programs" id="benchmarking-external-programs"><h1>Benchmarking External Programs</h1></a>
<p>Criterion.rs has the ability to benchmark external programs (which may be written in any language) the same way that it can benchmark rust functions. What follows is an example of how that can be done and some of the pitfalls to avoid along the way.</p>
<p>First, let's define our recursive Fibonacci function, only in Python this time:</p>
<pre><code class="language-python">def fibonacci(n):
    if n == 0 or n == 1:
        return 1
    return fibonacci(n-1) + fibonacci(n-2)
</code></pre>
<p>In order to benchmark this with Criterion.rs, we first need to write our own small benchmarking harness. I'll start with the complete code for this and then go over it in more detail:</p>
<pre><code class="language-python">import time
import sys

MILLIS = 1000
MICROS = MILLIS * 1000
NANOS = MICROS * 1000

def benchmark():
    argument = int(sys.argv[1])

    for line in sys.stdin:
        iters = int(line.strip())

        # Setup

        start = time.perf_counter()
        for x in range(iters):
            fibonacci(argument)
        end = time.perf_counter()

        # Teardown

        delta = end - start
        nanos = int(delta * NANOS)
        print(&quot;%d&quot; % nanos)
        sys.stdout.flush()

benchmark()
</code></pre>
<p>The important part is the <code>benchmark()</code> function.</p>
<a class="header" href="print.html#the-argument" id="the-argument"><h3>The Argument</h3></a>
<pre><code class="language-python">argument = int(sys.argv[1])
</code></pre>
<p>This example uses the <code>Criterion::bench_program_over_inputs</code> function to benchmark our Python Fibonacci function with a variety of inputs. The external program recieves the input value as a command-line argument appended to the command specified in the benchmark, so the very first thing our benchmark harness does is parse that argument into an integer. If we used <code>bench_program</code> instead, there would be no argument.</p>
<a class="header" href="print.html#reading-from-stdin" id="reading-from-stdin"><h3>Reading from stdin</h3></a>
<pre><code class="language-python">    for line in sys.stdin:
        iters = int(line.strip())
</code></pre>
<p>Next, our harness reads a line from stdin and parses it into an integer. Starting an external process is slow, and it would mess with our measurements if we had to do so for each iteration of the benchmark. Besides which, it would obscure the results (since we're probably more interested in the performance of the function without the process-creation overhead). Therefore, Criterion.rs starts the process once per input value or benchmark and sends the iteration counts to the external program on stdin. Your external benchmark harness must read and parse this iteration count and call the benchmarked function the appropriate number of times.</p>
<a class="header" href="print.html#setup" id="setup"><h3>Setup</h3></a>
<p>If your benchmarked code requires any setup, this is the time to do that.</p>
<a class="header" href="print.html#timing" id="timing"><h3>Timing</h3></a>
<pre><code class="language-python">        start = time.perf_counter()
        for x in range(iters):
            fibonacci(argument)
        end = time.perf_counter()
</code></pre>
<p>This is the heart of the external benchmark harness. We measure how long it takes to execute our Fibonacci function with the given argument in a loop, iterating the given number of times. It's important here to use the most precise timer available. We'll need to report the measurement in nanoseconds later, so if you can use a timer that returns a value in nanoseconds (eg. Java's <code>System.nanoTime()</code>) we can skip a bit of work later. It's OK if the timer can't measure to nanosecond precision (most PC's can't) but use the best timer you have.</p>
<a class="header" href="print.html#teardown" id="teardown"><h3>Teardown</h3></a>
<p>If your benchmarked code requires any teardown, this is the time to do that.</p>
<a class="header" href="print.html#reporting" id="reporting"><h3>Reporting</h3></a>
<pre><code class="language-python">        delta = end - start
        nanos = int(delta * NANOS)
        print(&quot;%d&quot; % nanos)
        sys.stdout.flush()
</code></pre>
<p>To report the measured time, simply print the elapsed number of nanoseconds to stdout. <code>perf_counter</code> reports its results as a floating-point number of seconds, so we first convert it to an integer number of nanoseconds before printing it.</p>
<p><strong>Beware Buffering:</strong> Criterion.rs will wait until it recieves the measurement before sending the next iteration count. If your benchmarks seem to be hanging during the warmup period, it may be because your benchmark harness is buffering the output on stdout, as Python does here. In this example we explicitly force Python to flush the buffer; you may need to do the same in your benchmarks.</p>
<a class="header" href="print.html#defining-the-benchmark" id="defining-the-benchmark"><h2>Defining the Benchmark</h2></a>
<p>If you've read the earlier pages, this will be quite familiar.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use criterion::Criterion;
use std::process::Command;

fn create_command() -&gt; Command {
    let mut command = Command::new(&quot;python3&quot;);
    command.arg(&quot;benches/external_process.py&quot;);
    command
}

fn python_fibonacci(c: &amp;mut Criterion) {
    c.bench_program_over_inputs(&quot;fibonacci-python&quot;,
        create_command,
        &amp;[1, 2, 4, 8, 16]);
}
#}</code></pre></pre>
<p>As before, we create a <code>Criterion</code> struct and use it to define our benchmark. This time, we use the <code>bench_program_over_inputs</code> method. This takes a function (used to create the <code>Command</code> which represents our external program) and an iterable containing the inputs to test. Aside from the use of a <code>Command</code> rather than a closure, this behaves just like (and produces the same output as) <code>bench_function_over_inputs</code>.</p>
<p>If your benchmark doesn't require input, simply omit the input values and use <code>bench_program</code> instead, which behaves like <code>bench_function</code>.</p>
<a class="header" href="print.html#analysis-process" id="analysis-process"><h1>Analysis Process</h1></a>
<p>This page details the data collection and analysis process used by Criterion.rs. This is a bit more advanced than the user guide; it is assumed the reader is somewhat familiar with statistical concepts. In particular, the reader should know what bootstrap sampling means.</p>
<p>So, without further ado, let's start with a general overview. Each benchmark in Criterion.rs goes through four phases:</p>
<ul>
<li>Warmup - The routine is executed repeatedly to fill the CPU and OS caches and (if applicable) give the JIT time to compile the code</li>
<li>Measurement - The routine is executed repeatedly and the execution times are recorded</li>
<li>Analysis - The recorded samples are analyzed and distilled into meaningful statistics, which are then reported to the user</li>
<li>Comparison - The performance of the current run is compared to the stored data from the last run to determine whether it has changed, and if so by how much</li>
</ul>
<a class="header" href="print.html#warmup-1" id="warmup-1"><h2>Warmup</h2></a>
<p>The first step in the process is warmup. In this phase, the routine is executed repeatedly to give the OS, CPU and JIT time to adapt to the new workload. This helps prevent things like cold caches and JIT compilation time from throwing off the measurements later. The warmup period is controlled by the <code>warm_up_time</code> value in the Criterion struct.</p>
<p>The warmup period is quite simple. The routine is executed once, then twice, four times and so on until the total accumulated execution time is greater than the configured warm up time. The number of iterations that were completed during this period is recorded, along with the elapsed time.</p>
<a class="header" href="print.html#measurement" id="measurement"><h2>Measurement</h2></a>
<p>The measurement phase is when Criterion.rs collects the performance data that will be analyzed and used in later stages. This phase is mainly controlled by the <code>measurement_time</code> value in the Criterion struct.</p>
<p>The measurements are done in a number of samples (see the <code>sample_size</code> parameter). Each sample consists of one or more (typically many) iterations of the routine. The elapsed time between the beginning and the end of the iterations, divided by the number of iterations, gives an estimate of the time taken by each iteration.</p>
<p>As measurement progresses, the sample iteration counts are increased. Suppose that the first sample contains 10 iterations. The second sample will contain 20, the third will contain 30 and so on. More formally, the iteration counts are calculated like so:</p>
<p><code>iterations = [d, 2d, 3d, ... Nd]</code></p>
<p>Where <code>N</code> is the total number of samples and <code>d</code> is a factor, calculated from the rough estimate of iteration time measured during the warmup period, which is used to scale the number of iterations to meet the configured measurement time. Note that <code>d</code> cannot be less than 1, and therefore the actual measurment time may exceed the configured measurement time if the iteration time is large or the configured measurement time is small.</p>
<p>Note that Criterion.rs does not measure each individual iteration, only the complete sample. The resulting samples are stored for use in later stages. The sample data is also written to the local disk so that it can be used in the comparison phase of future benchmark runs.</p>
<a class="header" href="print.html#analysis" id="analysis"><h2>Analysis</h2></a>
<p>During this phase Criterion.rs calculates useful statistics from the samples collected during the measurement phase.</p>
<a class="header" href="print.html#outlier-classification" id="outlier-classification"><h3>Outlier Classification</h3></a>
<p>The first step in analysis is outlier classification. Each sample is classified using a modified version of Tukey's Method, which will be summarized here. First, the interquartile range (IQR) is calculated from the difference between the 25th and 75th percentile. In Tukey's Method, values less than (25th percentile - 1.5 * IQR) or greater than (75th percentile + 1.5 * IQR) are considered outliers. Criterion.rs creates additional fences at (25pct - 3 * IQR) and (75pct + 3 * IQR); values outside that range are considered severe outliers.</p>
<p>Outlier classification is important because the analysis method used to estimate the average iteration time is sensitive to outliers. Thus, when Criterion.rs detects outliers, a warning is printed to inform the user that the benchmark may be less reliable. Additionally, a plot is generated showing which data points are considered outliers, where the fences are, etc.</p>
<p>Note, however, that outlier samples are <em>not</em> dropped from the data, and are used in the following analysis steps along with all other samples.</p>
<a class="header" href="print.html#linear-regression-1" id="linear-regression-1"><h3>Linear Regression</h3></a>
<p>The samples collected from a good benchmark should form a rough line when plotted on a chart showing the number of iterations and the time for each sample. The slope of that line gives an estimate of the time per iteration. A single estimate is difficult to interpret, however, since it contains no context. A confidence interval is generally more helpful. In order to generate a confidence interval, a large number of bootstrap samples are generated from the measured samples. A line is fitted to each of the bootstrap samples, and the result is a statistical distribution of slopes that gives a reliable confidence interval around the single estimate calculated from the measured samples.</p>
<p>This resampling process is repeated to generate the mean, standard deviation, median and median absolute deviation of the measured iteration times as well. All of this information is printed to the user and charts are generated. Finally, if there are saved statistics from a previous run, the two benchmark runs are compared.</p>
<a class="header" href="print.html#comparison" id="comparison"><h2>Comparison</h2></a>
<p>In the comparison phase, the statistics calculated from the current benchmark run are compared against those saved by the previous run to determine if the performance has changed in the meantime, and if so, by how much.</p>
<p>Once again, Criterion.rs generates many bootstrap samples, based on the measured samples from the two runs. The new and old bootstrap samples are compared and their T score is calculated using a T-test. The fraction of the bootstrapped T scores which are more extreme than the T score calculated by comparing the two measured samples gives the probability that the observed difference between the two sets of samples is merely by chance. Thus, if that probability is very low or zero, Criterion.rs can be confident that there is truly a difference in execution time between the two samples. In that case, the mean and median differences are bootstrapped and printed for the user, and the entire process begins again with the next benchmark.</p>
<p>This process can be extremely sensitive to changes, especially when combined with a small, highly deterministic benchmark routine. In these circumstances, even very small changes, such as differences in the load from background processes can change the measurements enough that the comparison process detects an optimization or regression. Since these sorts of unpredictable fluctuations are rarely of interest while benchmarking, there is also a configurable noise threshold. Optimizations or regressions within (for example) +-1% are considered noise and ignored. It is best to benchmark on a quiet computer where possible to minimize this noise, but it is not always possible to eliminate it entirely.</p>

                </div>

                <!-- Mobile navigation buttons -->
                

                

            </div>

            

            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <!-- Livereload script (if served using the cli tool) -->
        

        

        

        
        <script>
            $(document).ready(function() {
                window.print();
            })
        </script>
        

        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS script -->
        

    </body>
</html>
